{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 📝 Assignment – Persian NLP Toolkit Playground\n",
        "\n",
        "Welcome!  \n",
        "In this notebook you’ll experiment with **Hazm**, **Stanza‑fa**, **Parsivar**, **BERT‑fa**, and **GPT2‑fa**.  \n",
        "Follow the steps below and **replace every `.......` with your own code / answer**."
      ],
      "metadata": {
        "id": "uPXAS8qEeLiY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "outputs": [],
      "source": [
        "!pip install parsivar hazm stanza"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import fatal\n",
        "# ✨ Normalizer quick‑compare — Hazm, Parsivar, Stanza‑fa\n",
        "text = \"مي‌روم   به تهران 123٤٥۶!\"\n",
        "\n",
        "# 1) Hazm ------------------------------------------------------------\n",
        "from hazm import Normalizer as HazmNormalizer\n",
        "hazm_norm = HazmNormalizer(remove_diacritics=True)\n",
        "clean_hazm = hazm_norm.normalize(text)\n",
        "print(\"Hazm →\", clean_hazm)\n",
        "\n",
        "# 2) Parsivar --------------------------------------------------------\n",
        "from parsivar import Normalizer as ParsivarNormalizer\n",
        "parsivar_norm = ParsivarNormalizer(statistical_space_correction=True)\n",
        "clean_parsivar = parsivar_norm.normalize(text)\n",
        "print(\"Parsivar →\", clean_parsivar)\n",
        "\n",
        "# 3) Stanza‑fa (normalisation occurs inside the tokenizer) ----------\n",
        "import stanza\n",
        "stz = stanza.Pipeline(\"fa\", processors=\"tokenize\", use_gpu=False, verbose=False)\n",
        "doc = stz(text)\n",
        "stanza_tokens = [w.text for s in doc.sentences for w in s.words]\n",
        "print(\"Stanza tokens →\", stanza_tokens)\n",
        "print(\"Stanza (normalised join) →\", \" \".join(stanza_tokens))\n"
      ],
      "metadata": {
        "id": "LRrytqgD7MHv",
        "outputId": "df22e69e-e5a5-48fa-b9ea-2a9b032ac584",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hazm → می‌روم به تهران ۱۲۳۴۵۶!\n",
            "Parsivar → می‌روم به تهران 1234٥6 ! \n",
            "Stanza tokens → ['مي\\u200cروم', 'به', 'تهران', '123٤٥۶', '!']\n",
            "Stanza (normalised join) → مي‌روم به تهران 123٤٥۶ !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔹 Quick word‑tokenizer showcase — Hazm, Parsivar, Stanza‑fa\n",
        "sample = \"کتاب‌هایمان را به او می‌دهند.\"\n",
        "\n",
        "# 1) Hazm ------------------------------------------------------------\n",
        "from hazm import word_tokenize\n",
        "print(\"Hazm tokens:\", word_tokenize(sample))\n",
        "\n",
        "# 2) Parsivar --------------------------------------------------------\n",
        "from parsivar import Tokenizer\n",
        "p_tokenizer = Tokenizer()\n",
        "print(\"Parsivar tokens:\", p_tokenizer.tokenize_words(sample))\n",
        "\n",
        "# 3) Stanza‑fa -------------------------------------------------------\n",
        "import stanza\n",
        "nlp = stanza.Pipeline(\"fa\", processors=\"tokenize\", use_gpu=False, verbose=False)\n",
        "doc = nlp(sample)\n",
        "stanza_tokens = [w.text for s in doc.sentences for w in s.words]\n",
        "print(\"Stanza tokens:\", stanza_tokens)\n"
      ],
      "metadata": {
        "id": "DAIF6ilc7rys",
        "outputId": "d65f4009-eb78-4652-f4c1-173c44ab7365",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hazm tokens: ['کتاب\\u200cهایمان', 'را', 'به', 'او', 'می\\u200cدهند', '.']\n",
            "Parsivar tokens: ['کتاب\\u200cهایمان', 'را', 'به', 'او', 'می\\u200cدهند.']\n",
            "Stanza tokens: ['کتاب\\u200cهای', 'مان', 'را', 'به', 'او', 'می\\u200cدهند', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Part‑of‑Speech (POS) Tagging\n",
        "sample = \"کتاب‌هایمان را به او می‌دهند.\"\n",
        "\n",
        "# -------- Hazm POS --------\n",
        "!wget https://github.com/roshan-research/hazm/releases/download/v0.5/resources-0.5.zip\n",
        "!unzip resources-0.5.zip\n",
        "\n",
        "\n",
        "from hazm import Normalizer, word_tokenize, POSTagger\n",
        "normalizer = Normalizer()\n",
        "tokens_hazm = word_tokenize(normalizer.normalize(sample))\n",
        "pos_hazm = POSTagger(model='/content/postagger.model').tag(tokens_hazm)\n",
        "print(\"Hazm POS:\", pos_hazm)\n",
        "\n",
        "# -------- Parsivar POS --------\n",
        "from parsivar import Normalizer as PVNorm, Tokenizer, FindPOS\n",
        "pv_norm   = PVNorm()\n",
        "pv_tokens = Tokenizer().tokenize_words(pv_norm.normalize(sample))\n",
        "pos_parsi = FindPOS().parse_tokens(\" \".join(pv_tokens))\n",
        "print(\"Parsivar POS:\", pos_parsi)\n",
        "\n",
        "# -------- Stanza‑fa POS --------\n",
        "import stanza\n",
        "stz = stanza.Pipeline(\"fa\", processors=\"tokenize,pos\", use_gpu=False, verbose=False)\n",
        "doc = stz(sample)\n",
        "pos_stanza = [(w.text, w.upos) for s in doc.sentences for w in s.words]\n",
        "print(\"Stanza POS:\", pos_stanza)\n"
      ],
      "metadata": {
        "id": "vLKwH-vR6p7J",
        "outputId": "391e36a6-5910-4cd4-ffea-1466bfefeea4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stanza POS: [('کتاب\\u200cهای', 'NOUN'), ('مان', 'PRON'), ('را', 'ADP'), ('به', 'ADP'), ('او', 'PRON'), ('می\\u200cدهند', 'VERB'), ('.', 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming / Lemmatization\n",
        "sample = \"کتاب‌هایمان را به او می‌بخشند و دوباره پس می‌گیرند!\"\n",
        "\n",
        "# -------- Hazm stem / lemma --------\n",
        "from hazm import Stemmer, Lemmatizer, word_tokenize\n",
        "stemmer    = Stemmer()\n",
        "lemmatizer = Lemmatizer()\n",
        "print(\"Hazm stem:\",    [stemmer.stem(t)    for t in word_tokenize(sample)])\n",
        "print(\"Hazm lemma:\",   [lemmatizer.lemmatize(t) for t in word_tokenize(sample)])\n",
        "\n",
        "# -------- Parsivar stem / lemma --------\n",
        "\n",
        "from parsivar import FindStems, Tokenizer\n",
        "my_stemmer = FindStems()\n",
        "tokenizer = Tokenizer()\n",
        "tokens = tokenizer.tokenize_words(sample)\n",
        "stems_parsi = [my_stemmer.convert_to_stem(word) for word in tokens]\n",
        "print(\"Parsivar stem:\", stems_parsi)\n",
        "\n",
        "# Note: Parsivar doesn't have a direct lemmatizer in current version\n",
        "# As an alternative, we can use Stemmer for both stem and lemma\n",
        "print(\"Parsivar 'lemma' (using stem):\", stems_parsi)\n",
        "\n",
        "# -------- Stanza‑fa lemma --------\n",
        "import stanza\n",
        "stz = stanza.Pipeline(\"fa\", processors=\"tokenize,pos,lemma\", use_gpu=False, verbose=False)\n",
        "doc = stz(sample)\n",
        "lemmas_stanza = [w.lemma for s in doc.sentences for w in s.words]\n",
        "print(\"Stanza lemma:\", lemmas_stanza)\n",
        "\n"
      ],
      "metadata": {
        "id": "UNaZZ5tZO0mF",
        "outputId": "283e61fd-4e73-4276-cd80-2d8adb927c33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hazm stem: ['کتاب', 'را', 'به', 'او', 'می\\u200cبخشند', 'و', 'دوباره', 'پس', 'می\\u200cگیرند', '!']\n",
            "Hazm lemma: ['کتاب', 'را', 'به', 'او', 'بخشید#بخش', 'و', 'دوباره', 'پس', 'گرفت#گیر', '!']\n",
            "Parsivar stem: ['کتاب', 'را', 'به', 'او', 'بخشید&بخش', 'و', 'دوباره', 'پس', 'می\\u200cگیرند!']\n",
            "Parsivar 'lemma' (using stem): ['کتاب', 'را', 'به', 'او', 'بخشید&بخش', 'و', 'دوباره', 'پس', 'می\\u200cگیرند!']\n",
            "Stanza lemma: ['کتاب', 'ما', 'را', 'به', 'او', 'بخشید', 'و', 'دوباره', 'پس', 'گرفت', '!']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}