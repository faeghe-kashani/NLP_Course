{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“ AssignmentÂ â€“ Persian NLP Toolkit Playground\n",
        "\n",
        "Welcome!  \n",
        "In this notebook youâ€™ll experiment with **Hazm**, **Stanzaâ€‘fa**, **Parsivar**, **BERTâ€‘fa**, and **GPT2â€‘fa**.  \n",
        "Follow the steps below and **replace every `.......` with your own codeâ€¯/â€¯answer**."
      ],
      "metadata": {
        "id": "uPXAS8qEeLiY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "outputs": [],
      "source": [
        "!pip install parsivar hazm stanza"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import fatal\n",
        "# âœ¨ Normalizer quickâ€‘compare â€” Hazm, Parsivar, Stanzaâ€‘fa\n",
        "text = \"Ù…ÙŠâ€ŒØ±ÙˆÙ…   Ø¨Ù‡ ØªÙ‡Ø±Ø§Ù† 123Ù¤Ù¥Û¶!\"\n",
        "\n",
        "# 1) Hazm ------------------------------------------------------------\n",
        "from hazm import Normalizer as HazmNormalizer\n",
        "hazm_norm = HazmNormalizer(remove_diacritics=True)\n",
        "clean_hazm = hazm_norm.normalize(text)\n",
        "print(\"Hazm â†’\", clean_hazm)\n",
        "\n",
        "# 2) Parsivar --------------------------------------------------------\n",
        "from parsivar import Normalizer as ParsivarNormalizer\n",
        "parsivar_norm = ParsivarNormalizer(statistical_space_correction=True)\n",
        "clean_parsivar = parsivar_norm.normalize(text)\n",
        "print(\"Parsivar â†’\", clean_parsivar)\n",
        "\n",
        "# 3) Stanzaâ€‘fa (normalisation occurs inside the tokenizer) ----------\n",
        "import stanza\n",
        "stz = stanza.Pipeline(\"fa\", processors=\"tokenize\", use_gpu=False, verbose=False)\n",
        "doc = stz(text)\n",
        "stanza_tokens = [w.text for s in doc.sentences for w in s.words]\n",
        "print(\"Stanza tokens â†’\", stanza_tokens)\n",
        "print(\"StanzaÂ (normalised join) â†’\", \" \".join(stanza_tokens))\n"
      ],
      "metadata": {
        "id": "LRrytqgD7MHv",
        "outputId": "df22e69e-e5a5-48fa-b9ea-2a9b032ac584",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hazm â†’ Ù…ÛŒâ€ŒØ±ÙˆÙ… Ø¨Ù‡ ØªÙ‡Ø±Ø§Ù† Û±Û²Û³Û´ÛµÛ¶!\n",
            "Parsivar â†’ Ù…ÛŒâ€ŒØ±ÙˆÙ… Ø¨Ù‡ ØªÙ‡Ø±Ø§Ù† 1234Ù¥6 ! \n",
            "Stanza tokens â†’ ['Ù…ÙŠ\\u200cØ±ÙˆÙ…', 'Ø¨Ù‡', 'ØªÙ‡Ø±Ø§Ù†', '123Ù¤Ù¥Û¶', '!']\n",
            "StanzaÂ (normalised join) â†’ Ù…ÙŠâ€ŒØ±ÙˆÙ… Ø¨Ù‡ ØªÙ‡Ø±Ø§Ù† 123Ù¤Ù¥Û¶ !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ”¹ Quick wordâ€‘tokenizer showcase â€” Hazm, Parsivar, Stanzaâ€‘fa\n",
        "sample = \"Ú©ØªØ§Ø¨â€ŒÙ‡Ø§ÛŒÙ…Ø§Ù† Ø±Ø§ Ø¨Ù‡ Ø§Ùˆ Ù…ÛŒâ€ŒØ¯Ù‡Ù†Ø¯.\"\n",
        "\n",
        "# 1) Hazm ------------------------------------------------------------\n",
        "from hazm import word_tokenize\n",
        "print(\"Hazm tokens:\", word_tokenize(sample))\n",
        "\n",
        "# 2) Parsivar --------------------------------------------------------\n",
        "from parsivar import Tokenizer\n",
        "p_tokenizer = Tokenizer()\n",
        "print(\"Parsivar tokens:\", p_tokenizer.tokenize_words(sample))\n",
        "\n",
        "# 3) Stanzaâ€‘fa -------------------------------------------------------\n",
        "import stanza\n",
        "nlp = stanza.Pipeline(\"fa\", processors=\"tokenize\", use_gpu=False, verbose=False)\n",
        "doc = nlp(sample)\n",
        "stanza_tokens = [w.text for s in doc.sentences for w in s.words]\n",
        "print(\"Stanza tokens:\", stanza_tokens)\n"
      ],
      "metadata": {
        "id": "DAIF6ilc7rys",
        "outputId": "d65f4009-eb78-4652-f4c1-173c44ab7365",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hazm tokens: ['Ú©ØªØ§Ø¨\\u200cÙ‡Ø§ÛŒÙ…Ø§Ù†', 'Ø±Ø§', 'Ø¨Ù‡', 'Ø§Ùˆ', 'Ù…ÛŒ\\u200cØ¯Ù‡Ù†Ø¯', '.']\n",
            "Parsivar tokens: ['Ú©ØªØ§Ø¨\\u200cÙ‡Ø§ÛŒÙ…Ø§Ù†', 'Ø±Ø§', 'Ø¨Ù‡', 'Ø§Ùˆ', 'Ù…ÛŒ\\u200cØ¯Ù‡Ù†Ø¯.']\n",
            "Stanza tokens: ['Ú©ØªØ§Ø¨\\u200cÙ‡Ø§ÛŒ', 'Ù…Ø§Ù†', 'Ø±Ø§', 'Ø¨Ù‡', 'Ø§Ùˆ', 'Ù…ÛŒ\\u200cØ¯Ù‡Ù†Ø¯', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Partâ€‘ofâ€‘Speech (POS) Tagging\n",
        "sample = \"Ú©ØªØ§Ø¨â€ŒÙ‡Ø§ÛŒÙ…Ø§Ù† Ø±Ø§ Ø¨Ù‡ Ø§Ùˆ Ù…ÛŒâ€ŒØ¯Ù‡Ù†Ø¯.\"\n",
        "\n",
        "# -------- Hazm POS --------\n",
        "!wget https://github.com/roshan-research/hazm/releases/download/v0.5/resources-0.5.zip\n",
        "!unzip resources-0.5.zip\n",
        "\n",
        "\n",
        "from hazm import Normalizer, word_tokenize, POSTagger\n",
        "normalizer = Normalizer()\n",
        "tokens_hazm = word_tokenize(normalizer.normalize(sample))\n",
        "pos_hazm = POSTagger(model='/content/postagger.model').tag(tokens_hazm)\n",
        "print(\"Hazm POS:\", pos_hazm)\n",
        "\n",
        "# -------- Parsivar POS --------\n",
        "from parsivar import Normalizer as PVNorm, Tokenizer, FindPOS\n",
        "pv_norm   = PVNorm()\n",
        "pv_tokens = Tokenizer().tokenize_words(pv_norm.normalize(sample))\n",
        "pos_parsi = FindPOS().parse_tokens(\" \".join(pv_tokens))\n",
        "print(\"Parsivar POS:\", pos_parsi)\n",
        "\n",
        "# -------- Stanzaâ€‘fa POS --------\n",
        "import stanza\n",
        "stz = stanza.Pipeline(\"fa\", processors=\"tokenize,pos\", use_gpu=False, verbose=False)\n",
        "doc = stz(sample)\n",
        "pos_stanza = [(w.text, w.upos) for s in doc.sentences for w in s.words]\n",
        "print(\"Stanza POS:\", pos_stanza)\n"
      ],
      "metadata": {
        "id": "vLKwH-vR6p7J",
        "outputId": "391e36a6-5910-4cd4-ffea-1466bfefeea4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stanza POS: [('Ú©ØªØ§Ø¨\\u200cÙ‡Ø§ÛŒ', 'NOUN'), ('Ù…Ø§Ù†', 'PRON'), ('Ø±Ø§', 'ADP'), ('Ø¨Ù‡', 'ADP'), ('Ø§Ùˆ', 'PRON'), ('Ù…ÛŒ\\u200cØ¯Ù‡Ù†Ø¯', 'VERB'), ('.', 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#StemmingÂ /Â Lemmatization\n",
        "sample = \"Ú©ØªØ§Ø¨â€ŒÙ‡Ø§ÛŒÙ…Ø§Ù† Ø±Ø§ Ø¨Ù‡ Ø§Ùˆ Ù…ÛŒâ€ŒØ¨Ø®Ø´Ù†Ø¯ Ùˆ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ù¾Ø³ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù†Ø¯!\"\n",
        "\n",
        "# -------- Hazm stem / lemma --------\n",
        "from hazm import Stemmer, Lemmatizer, word_tokenize\n",
        "stemmer    = Stemmer()\n",
        "lemmatizer = Lemmatizer()\n",
        "print(\"Hazm stem:\",    [stemmer.stem(t)    for t in word_tokenize(sample)])\n",
        "print(\"Hazm lemma:\",   [lemmatizer.lemmatize(t) for t in word_tokenize(sample)])\n",
        "\n",
        "# -------- Parsivar stem / lemma --------\n",
        "\n",
        "from parsivar import FindStems, Tokenizer\n",
        "my_stemmer = FindStems()\n",
        "tokenizer = Tokenizer()\n",
        "tokens = tokenizer.tokenize_words(sample)\n",
        "stems_parsi = [my_stemmer.convert_to_stem(word) for word in tokens]\n",
        "print(\"Parsivar stem:\", stems_parsi)\n",
        "\n",
        "# Note: Parsivar doesn't have a direct lemmatizer in current version\n",
        "# As an alternative, we can use Stemmer for both stem and lemma\n",
        "print(\"Parsivar 'lemma' (using stem):\", stems_parsi)\n",
        "\n",
        "# -------- Stanzaâ€‘fa lemma --------\n",
        "import stanza\n",
        "stz = stanza.Pipeline(\"fa\", processors=\"tokenize,pos,lemma\", use_gpu=False, verbose=False)\n",
        "doc = stz(sample)\n",
        "lemmas_stanza = [w.lemma for s in doc.sentences for w in s.words]\n",
        "print(\"Stanza lemma:\", lemmas_stanza)\n",
        "\n"
      ],
      "metadata": {
        "id": "UNaZZ5tZO0mF",
        "outputId": "283e61fd-4e73-4276-cd80-2d8adb927c33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hazm stem: ['Ú©ØªØ§Ø¨', 'Ø±Ø§', 'Ø¨Ù‡', 'Ø§Ùˆ', 'Ù…ÛŒ\\u200cØ¨Ø®Ø´Ù†Ø¯', 'Ùˆ', 'Ø¯ÙˆØ¨Ø§Ø±Ù‡', 'Ù¾Ø³', 'Ù…ÛŒ\\u200cÚ¯ÛŒØ±Ù†Ø¯', '!']\n",
            "Hazm lemma: ['Ú©ØªØ§Ø¨', 'Ø±Ø§', 'Ø¨Ù‡', 'Ø§Ùˆ', 'Ø¨Ø®Ø´ÛŒØ¯#Ø¨Ø®Ø´', 'Ùˆ', 'Ø¯ÙˆØ¨Ø§Ø±Ù‡', 'Ù¾Ø³', 'Ú¯Ø±ÙØª#Ú¯ÛŒØ±', '!']\n",
            "Parsivar stem: ['Ú©ØªØ§Ø¨', 'Ø±Ø§', 'Ø¨Ù‡', 'Ø§Ùˆ', 'Ø¨Ø®Ø´ÛŒØ¯&Ø¨Ø®Ø´', 'Ùˆ', 'Ø¯ÙˆØ¨Ø§Ø±Ù‡', 'Ù¾Ø³', 'Ù…ÛŒ\\u200cÚ¯ÛŒØ±Ù†Ø¯!']\n",
            "Parsivar 'lemma' (using stem): ['Ú©ØªØ§Ø¨', 'Ø±Ø§', 'Ø¨Ù‡', 'Ø§Ùˆ', 'Ø¨Ø®Ø´ÛŒØ¯&Ø¨Ø®Ø´', 'Ùˆ', 'Ø¯ÙˆØ¨Ø§Ø±Ù‡', 'Ù¾Ø³', 'Ù…ÛŒ\\u200cÚ¯ÛŒØ±Ù†Ø¯!']\n",
            "Stanza lemma: ['Ú©ØªØ§Ø¨', 'Ù…Ø§', 'Ø±Ø§', 'Ø¨Ù‡', 'Ø§Ùˆ', 'Ø¨Ø®Ø´ÛŒØ¯', 'Ùˆ', 'Ø¯ÙˆØ¨Ø§Ø±Ù‡', 'Ù¾Ø³', 'Ú¯Ø±ÙØª', '!']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}