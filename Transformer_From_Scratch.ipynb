{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "60715629",
      "metadata": {
        "id": "60715629"
      },
      "source": [
        "# Transformer from Scratch: *Attention Is All You Need* üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d14a11b",
      "metadata": {
        "id": "1d14a11b"
      },
      "source": [
        "This notebook implements a Transformer model from scratch in PyTorch, based on the paper 'Attention Is All You Need'.\n",
        "\n",
        "\n",
        "**Note:**  if you're:\n",
        "\n",
        "- Downloading a dataset from the internet using commands like `wget`, `curl`, or Python libraries\n",
        "- Loading a dataset from repositories like *Kaggle*, *Hugging Face*, or *TensorFlow* Datasets\n",
        "- Using *API* calls to fetch data\n",
        "\n",
        "All of these operations use Google's internet connection and are downloaded directly to the Colab virtual machine. The data is stored temporarily in the Colab instance's storage space, not on your local machine."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries if not already installed\n",
        "!pip install datasets sentencepiece\n",
        "!pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "!pip install torchtext==0.15.1\n"
      ],
      "metadata": {
        "id": "JP8QW7CxoGyp"
      },
      "id": "JP8QW7CxoGyp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk.translate.bleu_score as bleu\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "NOxkC715wdC7"
      },
      "id": "NOxkC715wdC7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "full_dataset = load_dataset(\"opus_books\", \"en-nl\", split=\"train\")  # or iwslt2017\n",
        "\n",
        "SRC_LANGUAGE = \"nl\"\n",
        "TGT_LANGUAGE = \"en\"\n",
        "\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "\n",
        "def build_vocab(dataset, lang):\n",
        "    counter = Counter()\n",
        "    for item in dataset:\n",
        "        counter.update(word_tokenize(item['translation'][lang]))\n",
        "    vocab = {sym: i for i, sym in enumerate(special_symbols)}\n",
        "    for word in counter:\n",
        "        if word not in vocab:\n",
        "            vocab[word] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "# Build vocab on full data (or just training set)\n",
        "src_vocab = build_vocab(full_dataset, \"nl\")\n",
        "tgt_vocab = build_vocab(full_dataset, \"en\")\n"
      ],
      "metadata": {
        "id": "cwWmsMfy7b8G"
      },
      "id": "cwWmsMfy7b8G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tokenize and index helpers\n",
        "def tokenize(text):\n",
        "    return word_tokenize(text.lower())\n",
        "\n",
        "def encode(text, vocab):\n",
        "    return [vocab.get(tok, UNK_IDX) for tok in tokenize(text)]\n",
        "\n",
        "# Collate function to prepare batches\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for item in batch:\n",
        "        # Tokenize and numerify source sentence (Dutch)\n",
        "        src_indices = encode(item['translation'][SRC_LANGUAGE], src_vocab)\n",
        "        src_tensor = torch.tensor([BOS_IDX] + src_indices + [EOS_IDX], dtype=torch.long)\n",
        "        src_batch.append(src_tensor)\n",
        "\n",
        "        # Tokenize and numerify target sentence (English)\n",
        "        tgt_indices = encode(item['translation'][TGT_LANGUAGE], tgt_vocab)\n",
        "        tgt_tensor = torch.tensor([BOS_IDX] + tgt_indices + [EOS_IDX], dtype=torch.long)\n",
        "        tgt_batch.append(tgt_tensor)\n",
        "\n",
        "    # Pad sequences\n",
        "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch\n"
      ],
      "metadata": {
        "id": "wXgun3KGlE-v"
      },
      "id": "wXgun3KGlE-v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Create matrix of shape (max_len, d_model) with positional encoding vectors\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # [max_len, 1]\n",
        "        # Compute the div term: 10000^{-2i/d_model} for even i\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))\n",
        "        # Apply sin to even indices, cos to odd indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # 0,2,4,... dims\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # 1,3,5,... dims\n",
        "        pe = pe.unsqueeze(0)  # shape (1, max_len, d_model) to allow broadcasting\n",
        "        self.register_buffer('pe', pe)  # register as buffer so it‚Äôs saved with model and on same device\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x shape: (batch_size, seq_len, d_model)\n",
        "        seq_len = x.size(1)\n",
        "        # Add positional encoding up to seq_len\n",
        "        x = x + self.pe[:, :seq_len, :]\n",
        "        return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "DstFLPx6lHGV"
      },
      "id": "DstFLPx6lHGV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Instantiate PositionalEncoding for a small d_model (e.g., 16) to visualize\n",
        "pos_enc = PositionalEncoding(d_model=16, dropout=0.0, max_len=100)\n",
        "pos_enc_values = pos_enc.pe[0, :50, :].cpu().tolist()  # first 50 positions, all 16 dims\n",
        "\n",
        "plt.figure(figsize=(6,3))\n",
        "# Plot positional encoding for dimension 0 and 1 (index 0: sin wave, index 1: cos wave)\n",
        "pos_enc_values = np.array(pos_enc_values)\n",
        "\n",
        "plt.plot(pos_enc_values[:, 0], label=\"Dimension 0 (sin)\")\n",
        "plt.plot(pos_enc_values[:, 1], label=\"Dimension 1 (cos)\")\n",
        "plt.title(\"Positional Encoding Example (Dimensions 0 and 1)\")\n",
        "plt.xlabel(\"Position\")\n",
        "plt.ylabel(\"PE Value\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Z3WrPdS2lMwx"
      },
      "id": "Z3WrPdS2lMwx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def scaled_dot_product_attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask: torch.Tensor = None):\n",
        "    # Q, K, V shape: (batch, heads, seq_len_q, d_k), (batch, heads, seq_len_k, d_k), ...\n",
        "    d_k = Q.size(-1)\n",
        "    # 1. Compute raw attention scores by dot product\n",
        "    # scores shape: (batch, heads, seq_len_q, seq_len_k)\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    # 2. Apply mask (if provided) by setting scores to -inf where mask is True\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(~mask, -1e9)\n",
        "\n",
        "    # Subtract the max for numerical stability.\n",
        "    scores = scores - scores.max(dim=-1, keepdim=True)[0]\n",
        "    # 3. Softmax to get attention weights\n",
        "    attn_weights = F.softmax(scores, dim=-1)  # along seq_len_k\n",
        "    # 4. Use weights to get weighted sum of values\n",
        "    output = torch.matmul(attn_weights, V)  # shape: (batch, heads, seq_len_q, d_k)\n",
        "    return output, attn_weights\n"
      ],
      "metadata": {
        "id": "9DVgwOCslOsj"
      },
      "id": "9DVgwOCslOsj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "\n",
        "        # Learned projection matrices\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # For analysis, we can store attention weights if needed\n",
        "        self.attn_weights = None\n",
        "\n",
        "    def forward(self, Q_in: torch.Tensor, K_in: torch.Tensor, V_in: torch.Tensor, mask: torch.Tensor = None):\n",
        "        batch_size = Q_in.size(0)\n",
        "        # 1. Linear projections\n",
        "        Q = self.W_q(Q_in)  # shape: (batch, seq_len_q, d_model)\n",
        "        K = self.W_k(K_in)  # shape: (batch, seq_len_k, d_model)\n",
        "        V = self.W_v(V_in)  # shape: (batch, seq_len_k, d_model)\n",
        "        # 2. Reshape and split into heads\n",
        "        # New shape: (batch, seq_len_q, n_heads, d_k) then permute\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.d_k).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.d_k).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.d_k).permute(0, 2, 1, 3)\n",
        "        # 3. Apply scaled dot-product attention (with heads as batch dimension)\n",
        "        if mask is not None:\n",
        "            # mask shape expected: (batch, 1, seq_q, seq_k) or (batch, n_heads, seq_q, seq_k)\n",
        "            # Expand mask to include head dimension if needed\n",
        "            if mask.size(1) == 1 and mask.size(0) == batch_size:\n",
        "                mask = mask.expand(batch_size, self.n_heads, mask.size(2), mask.size(3))\n",
        "        attn_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
        "        # 4. Concatenate heads and project\n",
        "        attn_output = attn_output.permute(0, 2, 1, 3).contiguous()  # back to (batch, seq_len_q, n_heads, d_k)\n",
        "        '''However, under the hood, permute() does not rearrange the actual data in memory.\n",
        "        It just gives you a view with a new dimension order (by changing how indices are interpreted),\n",
        "        which means the data is now non-contiguous in memory.'''\n",
        "\n",
        "        attn_output = attn_output.view(batch_size, -1, self.d_model)  # (batch, seq_len_q, d_model)\n",
        "        output = self.W_o(attn_output)\n",
        "        output = self.dropout(output)\n",
        "        # Store attention weights for possible visualization (concatenate heads or take first head)\n",
        "        self.attn_weights = attn_weights  # shape: (batch, n_heads, seq_len_q, seq_len_k)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "j07uUZSrlQiS"
      },
      "id": "j07uUZSrlQiS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x shape: (batch, seq_len, d_model)\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "B0RxzifqlSPg"
      },
      "id": "B0RxzifqlSPg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, src_mask: torch.Tensor = None) -> torch.Tensor:\n",
        "        # Self-attention sub-layer\n",
        "        attn_output = self.self_attn(x, x, x, mask=src_mask)\n",
        "        x = x + attn_output  # add residual\n",
        "        x = self.norm1(x)    # layer norm\n",
        "        # Feed-forward sub-layer\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = x + self.dropout(ffn_output)  # add residual\n",
        "        x = self.norm2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "2SRu4QZ8lUBH"
      },
      "id": "2SRu4QZ8lUBH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, enc_output: torch.Tensor, tgt_mask: torch.Tensor = None, mem_mask: torch.Tensor = None):\n",
        "        # 1. Decoder self-attention (mask out future tokens)\n",
        "        attn_output = self.self_attn(x, x, x, mask=tgt_mask)\n",
        "        x = x + attn_output\n",
        "        x = self.norm1(x)\n",
        "        # 2. Encoder-Decoder cross-attention (queries = x, keys/values = enc_output)\n",
        "        attn_output = self.cross_attn(x, enc_output, enc_output, mask=mem_mask)\n",
        "        x = x + attn_output\n",
        "        x = self.norm2(x)\n",
        "        # 3. Position-wise feed-forward\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = x + self.dropout(ffn_output)\n",
        "        x = self.norm3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Bu0ICAZalVht"
      },
      "id": "Bu0ICAZalVht",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, src_vocab_size: int, tgt_vocab_size: int, d_model: int = 256, n_heads: int = 4,\n",
        "                 num_encoder_layers: int = 3, num_decoder_layers: int = 3, d_ff: int = 512, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # Embedding layers\n",
        "        self.src_emb = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
        "        # Encoder and Decoder stacks\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_encoder_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_decoder_layers)])\n",
        "        # Final output linear layer\n",
        "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "    def encode(self, src: torch.Tensor, src_mask: torch.Tensor = None):\n",
        "        # src: [batch, src_len] (token indices)\n",
        "        x = self.src_emb(src) * math.sqrt(self.d_model)\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x, src_mask)\n",
        "        return x  # shape: [batch, src_len, d_model]\n",
        "\n",
        "    def decode(self, tgt: torch.Tensor, enc_output: torch.Tensor, tgt_mask: torch.Tensor = None, mem_mask: torch.Tensor = None):\n",
        "        # tgt: [batch, tgt_len], enc_output: [batch, src_len, d_model]\n",
        "        x = self.tgt_emb(tgt) * math.sqrt(self.d_model)\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.decoder_layers:\n",
        "            x = layer(x, enc_output, tgt_mask, mem_mask)\n",
        "        return x  # shape: [batch, tgt_len, d_model]\n",
        "\n",
        "    def forward(self, src: torch.Tensor, tgt: torch.Tensor, src_mask: torch.Tensor = None,\n",
        "                tgt_mask: torch.Tensor = None, mem_mask: torch.Tensor = None):\n",
        "        enc_output = self.encode(src, src_mask)\n",
        "        dec_output = self.decode(tgt, enc_output, tgt_mask, mem_mask)\n",
        "        logits = self.fc_out(dec_output)  # raw logits for each token in tgt sequence\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "UmjBewgGlXXd"
      },
      "id": "UmjBewgGlXXd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Instantiate a small transformer for testing\n",
        "test_model = TransformerModel(\n",
        "    src_vocab_size=len(src_vocab),\n",
        "    tgt_vocab_size=len(tgt_vocab),\n",
        "    d_model=32,\n",
        "    n_heads=4,\n",
        "    num_encoder_layers=2,\n",
        "    num_decoder_layers=2,\n",
        "    d_ff=64\n",
        ")\n",
        "\n",
        "# Dummy input batch (Dutch source and English target)\n",
        "dummy_src = torch.tensor([[BOS_IDX, 5, 6, 7, EOS_IDX, PAD_IDX, PAD_IDX]], dtype=torch.long)  # [1, 7]\n",
        "dummy_tgt = torch.tensor([[BOS_IDX, 10, 11, EOS_IDX, PAD_IDX]], dtype=torch.long)            # [1, 5]\n",
        "\n",
        "# Create masks for dummy input\n",
        "def generate_masks(src, tgt):\n",
        "    # No source future mask (can attend to all)\n",
        "    src_pad_mask = (src == PAD_IDX).unsqueeze(1).unsqueeze(2)  # shape [batch, 1, 1, src_len]\n",
        "\n",
        "    # Target future mask (prevent looking ahead)\n",
        "    tgt_len = tgt.size(1)\n",
        "    future_mask = torch.triu(torch.ones((tgt_len, tgt_len), dtype=torch.bool), diagonal=1)\n",
        "    future_mask = future_mask.unsqueeze(0).unsqueeze(0)  # shape [1, 1, tgt_len, tgt_len]\n",
        "\n",
        "    # Target pad mask (prevent attending to <pad>)\n",
        "    tgt_pad_mask = (tgt == PAD_IDX).unsqueeze(1).unsqueeze(3)  # shape [batch, 1, tgt_len, 1]\n",
        "\n",
        "    # Combine both masks\n",
        "    tgt_mask = future_mask | tgt_pad_mask  # broadcasted to [batch, 1, tgt_len, tgt_len]\n",
        "\n",
        "    # Memory mask for encoder-decoder attention (prevent decoder from attending to <pad> in encoder)\n",
        "    mem_mask = src_pad_mask  # shape [batch, 1, 1, src_len]\n",
        "\n",
        "    return src_pad_mask, tgt_mask, mem_mask\n",
        "\n",
        "# Generate masks\n",
        "src_mask, tgt_mask, mem_mask = generate_masks(dummy_src, dummy_tgt)\n",
        "\n",
        "# Forward pass\n",
        "out_logits = test_model(dummy_src, dummy_tgt, src_mask, tgt_mask, mem_mask)\n",
        "\n",
        "# Output shape\n",
        "print(\"Output logits shape:\", out_logits.shape)  # Expecting [1, 5, tgt_vocab_size]\n"
      ],
      "metadata": {
        "id": "1-Kbr8_glZGM"
      },
      "id": "1-Kbr8_glZGM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Device configuration (use GPU if available)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Instantiate the Transformer model\n",
        "d_model = 256   # embedding dimension\n",
        "n_heads = 8     # number of attention heads\n",
        "d_ff = 512      # feed-forward inner layer dimension\n",
        "num_layers = 4  # number of encoder and decoder layers\n",
        "model = TransformerModel(len(src_vocab),\n",
        "                         len(tgt_vocab),\n",
        "                         d_model=d_model, n_heads=n_heads,\n",
        "                         num_encoder_layers=num_layers, num_decoder_layers=num_layers,\n",
        "                         d_ff=d_ff, dropout=0.1).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX, label_smoothing=0.1)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Function to create masks for a batch\n",
        "def create_masks(src: torch.Tensor, tgt: torch.Tensor):\n",
        "    \"\"\"\n",
        "    src: [batch, src_len], tgt: [batch, tgt_len] with BOS and EOS included in tgt.\n",
        "    Returns src_mask, tgt_mask, mem_mask.\n",
        "    \"\"\"\n",
        "    batch_size, src_len = src.size()\n",
        "    tgt_len = tgt.size(1)\n",
        "    # Encoder (src) padding mask: shape [batch, 1, 1, src_len]\n",
        "    src_pad_mask = (src == PAD_IDX).unsqueeze(1).unsqueeze(2)  # True where pad\n",
        "    # Decoder (tgt) padding mask for keys:\n",
        "    tgt_pad_mask = (tgt == PAD_IDX).unsqueeze(1).unsqueeze(3)  # [batch, 1, tgt_len, 1]\n",
        "    # Subsequent mask for decoder (to mask future positions)\n",
        "    # shape [tgt_len, tgt_len]: True where j>i\n",
        "    future_mask = torch.triu(torch.ones((tgt_len, tgt_len), device=device, dtype=torch.bool), diagonal=1)\n",
        "    future_mask = future_mask.unsqueeze(0).unsqueeze(0)  # [1, 1, tgt_len, tgt_len]\n",
        "    # Combine tgt masks: mask out future and padded keys\n",
        "    tgt_mask = future_mask | tgt_pad_mask  # broadcast over batch\n",
        "    # For decoder cross-attention: mask out source pads for each target query\n",
        "    mem_mask = src_pad_mask  # shape [batch, 1, 1, src_len] (will broadcast across tgt_len dimension in attention)\n",
        "    return src_pad_mask, tgt_mask, mem_mask\n",
        "\n",
        "# Training loop\n",
        "NUM_EPOCHS = 10\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src_batch, tgt_batch in train_loader:\n",
        "        src_batch = src_batch.to(device)\n",
        "        tgt_batch = tgt_batch.to(device)\n",
        "        # Prepare target input and output\n",
        "        tgt_in = tgt_batch[:, :-1]   # input to decoder (everything except last token)\n",
        "        tgt_out = tgt_batch[:, 1:]   # target output (everything except first token <BOS>)\n",
        "        # Create masks\n",
        "        src_mask, tgt_mask, mem_mask = create_masks(src_batch, tgt_in)\n",
        "        # Forward pass\n",
        "        logits = model(src_batch, tgt_in, src_mask, tgt_mask, mem_mask)\n",
        "        # Compute loss (flatten the logits and targets for cross-entropy)\n",
        "        loss = criterion(logits.view(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for src_batch, tgt_batch in val_loader:\n",
        "            src_batch = src_batch.to(device)\n",
        "            tgt_batch = tgt_batch.to(device)\n",
        "            tgt_in = tgt_batch[:, :-1]\n",
        "            tgt_out = tgt_batch[:, 1:]\n",
        "            src_mask, tgt_mask, mem_mask = create_masks(src_batch, tgt_in)\n",
        "            logits = model(src_batch, tgt_in, src_mask, tgt_mask, mem_mask)\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
        "            val_loss += loss.item()\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.3f}, Val Loss = {avg_val_loss:.3f}\")\n"
      ],
      "metadata": {
        "id": "LWDkACXdlbO9"
      },
      "id": "LWDkACXdlbO9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.translate.bleu_score as bleu\n",
        "\n",
        "# invert vocab for decoding\n",
        "inv_tgt_vocab = {v: k for k, v in tgt_vocab.items()}\n",
        "\n",
        "def greedy_decode(model, src_sentence, max_len=50):\n",
        "    model.eval()\n",
        "    src = torch.tensor(src_sentence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    src_mask = (src == PAD_IDX).unsqueeze(1).unsqueeze(2)\n",
        "    enc_output = model.encode(src, src_mask)\n",
        "\n",
        "    tgt_indices = [BOS_IDX]\n",
        "    for _ in range(max_len):\n",
        "        tgt = torch.tensor(tgt_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
        "        tgt_mask = torch.triu(\n",
        "            torch.ones((1, 1, tgt.size(1), tgt.size(1)), device=device, dtype=torch.bool),\n",
        "            diagonal=1\n",
        "        )\n",
        "        src_mask = (src == PAD_IDX).unsqueeze(1).unsqueeze(1)\n",
        "        mem_mask = src_mask.expand(-1, -1, tgt.size(1), -1)\n",
        "\n",
        "        out = model.decode(tgt, enc_output, tgt_mask, mem_mask)\n",
        "        logits = model.fc_out(out)\n",
        "        next_token = int(logits[0, -1, :].argmax(dim=-1).item())\n",
        "        tgt_indices.append(next_token)\n",
        "        if next_token == EOS_IDX:\n",
        "            break\n",
        "\n",
        "    return tgt_indices\n",
        "\n",
        "# pull out the first 100 translation‚Äêdicts\n",
        "first100 = val_dataset['translation'][:100]\n",
        "\n",
        "references = []\n",
        "candidates  = []\n",
        "\n",
        "for trans in first100:\n",
        "    src_text = trans[SRC_LANGUAGE]\n",
        "    tgt_text = trans[TGT_LANGUAGE]\n",
        "\n",
        "    # tokenize & encode source\n",
        "    src_tokens  = tokenize(src_text)\n",
        "    src_indices = [BOS_IDX] + [src_vocab.get(tok, UNK_IDX) for tok in src_tokens] + [EOS_IDX]\n",
        "\n",
        "    # greedy decode\n",
        "    pred_indices = greedy_decode(model, src_indices, max_len=100)\n",
        "\n",
        "    # convert to tokens (skip BOS, stop at EOS)\n",
        "    pred_tokens = []\n",
        "    for idx in pred_indices[1:]:\n",
        "        if idx == EOS_IDX:\n",
        "            break\n",
        "        pred_tokens.append(inv_tgt_vocab.get(idx, '<unk>'))\n",
        "\n",
        "    # reference tokens\n",
        "    ref_tokens = tokenize(tgt_text)\n",
        "\n",
        "    candidates.append(pred_tokens)\n",
        "    references.append([ref_tokens])\n",
        "\n",
        "# compute BLEU on those 100\n",
        "bleu_score = bleu.corpus_bleu(references, candidates)\n",
        "print(f\"BLEU score on first 100 sentences: {bleu_score:.3f}\")\n"
      ],
      "metadata": {
        "id": "nRsfXj77lc90"
      },
      "id": "nRsfXj77lc90",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1) Load & split\n",
        "raw           = load_dataset(\"opus_books\", \"en-nl\", split=\"train\")\n",
        "splits        = raw.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = splits[\"train\"]\n",
        "val_dataset   = splits[\"test\"]\n",
        "\n",
        "# You already have:\n",
        "#   SRC_LANGUAGE = \"nl\"; TGT_LANGUAGE = \"en\"\n",
        "#   tokenize = your nltk-based tokenizer\n",
        "#   src_vocab, tgt_vocab = your vocab dicts\n",
        "#   UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0,1,2,3\n",
        "inv_tgt_vocab = {idx: tok for tok, idx in tgt_vocab.items()}\n",
        "\n",
        "def translate_sentence(sentence: str, max_len: int = 50) -> str:\n",
        "    model.eval()\n",
        "\n",
        "    # 1) tokenize & numericalize Dutch\n",
        "    src_tokens  = tokenize(sentence)\n",
        "    src_indices = [src_vocab.get(tok, UNK_IDX) for tok in src_tokens]\n",
        "    src_indices = [BOS_IDX] + src_indices + [EOS_IDX]\n",
        "\n",
        "    # 2) greedy decode ‚Üí list of token‚Äêindices\n",
        "    pred_indices = greedy_decode(model, src_indices, max_len=max_len)\n",
        "\n",
        "    # 3) map back to English tokens\n",
        "    pred_tokens = []\n",
        "    for idx in pred_indices[1:]:\n",
        "        if idx == EOS_IDX:\n",
        "            break\n",
        "        pred_tokens.append(inv_tgt_vocab.get(idx, \"<unk>\"))\n",
        "\n",
        "    return \" \".join(pred_tokens)\n",
        "\n",
        "# 2) Run on your three custom examples\n",
        "examples = [\n",
        "    \"Een man in een blauw shirt staat op een ladder en wast een raam.\",\n",
        "    \"Twee kinderen spelen met een bal in het park.\",\n",
        "    \"Een jongen met groene broek rent naast een hond.\"\n",
        "]\n",
        "for dutch in examples:\n",
        "    print(f\"DUTCH:             {dutch}\")\n",
        "    print(f\"PREDICTED ENGLISH: {translate_sentence(dutch)}\")\n",
        "    print(\"-----\")\n",
        "\n",
        "# 3) Compute BLEU over the FIRST 100 validation examples\n",
        "references = []\n",
        "candidates = []\n",
        "\n",
        "first100 = val_dataset[\"translation\"][:100]\n",
        "for trans in first100:\n",
        "    src = trans[SRC_LANGUAGE]\n",
        "    tgt = trans[TGT_LANGUAGE]\n",
        "\n",
        "    # get prediction and tokenize both sides\n",
        "    pred = translate_sentence(src, max_len=100).split()\n",
        "    ref  = tokenize(tgt)\n",
        "\n",
        "    candidates.append(pred)\n",
        "    references.append([ref])  # list-of-refs for corpus_bleu\n",
        "\n",
        "bleu_score = bleu.corpus_bleu(references, candidates)\n",
        "print(f\"BLEU score on first 100 sentences: {bleu_score:.3f}\")\n"
      ],
      "metadata": {
        "id": "mrIlDbkIledk"
      },
      "id": "mrIlDbkIledk",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}