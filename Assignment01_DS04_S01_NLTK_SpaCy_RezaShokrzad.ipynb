{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b057fad5",
      "metadata": {
        "id": "b057fad5"
      },
      "source": [
        "# **Practice Assignment: NLP with NLTK & spaCy**\n",
        "\n",
        "* This assignment is part of the NLP Workshop on YouTube, which is free and open to the public.\n",
        "* **Lecturer: Reza Shokrzad.**\n",
        "*‌ [دسترسی به جلسه اول کلاس](https://youtube.com/live/lDCoqQSc4ZE?feature=share)\n",
        "* [برنامه اجرایی کلاس و جلسات](https://docs.google.com/spreadsheets/d/1SP3NJ9H7yp8sgof-zp_t4oxmdxjMdEgoL_mmCDvdUm4/edit?gid=0#gid=0)\n",
        "\n",
        "\n",
        "Welcome to this **Fill-in-the-Blanks NLP Assignment!** 🎯 This exercise will help you solidify your understanding of **NLTK** and **spaCy** by filling in the missing parts of the code. Follow the instructions carefully, and make sure to test your solutions!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa085b7f",
      "metadata": {
        "id": "aa085b7f"
      },
      "source": [
        "## **1. Working with Corpora & Lexical Resources**\n",
        "**Task:** Load and analyze texts from different corpora.\n",
        "- Use NLTK’s **Gutenberg** corpus to load the text of *Moby Dick*.\n",
        "- Tokenize it into words.\n",
        "- Count the top 10 most frequent words (excluding stopwords)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "757d9099",
      "metadata": {
        "id": "757d9099",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b1d0bbc-0ead-4599-8b8b-6230139a2ad5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('whale', 1095), ('one', 913), ('like', 580), ('upon', 565), ('ahab', 511), ('man', 498), ('ship', 469), ('old', 443), ('ye', 438), ('would', 436)]\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('gutenberg')\n",
        "\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "\n",
        "# Load text\n",
        "text = gutenberg.raw('melville-moby_dick.txt')  # FILL THIS\n",
        "\n",
        "# Tokenize words\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stopwords.words('english')]  # FILL THIS\n",
        "\n",
        "# Compute frequency distribution\n",
        "fdist = FreqDist(filtered_words)\n",
        "\n",
        "# Print top 10 words\n",
        "print(fdist.most_common(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2894d7a",
      "metadata": {
        "id": "f2894d7a"
      },
      "source": [
        "## **2. Tokenization Techniques**\n",
        "**Task:** Tokenize a given text using both **NLTK** and **spaCy**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "85a9ac50",
      "metadata": {
        "id": "85a9ac50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f25a7761-804d-46f2-8b3d-5a4a81e73621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "NLTK Word Tokens: ['SpaCy', 'is', 'fast', '!', 'However', ',', 'NLTK', 'provides', 'flexibility', 'in', 'tokenization', '.']\n",
            "NLTK Sentence Tokens: ['SpaCy is fast!', 'However, NLTK provides flexibility in tokenization.']\n",
            "spaCy Tokens: ['SpaCy', 'is', 'fast', '!', 'However', ',', 'NLTK', 'provides', 'flexibility', 'in', 'tokenization', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "!pip install spacy\n",
        "import spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"SpaCy is fast! However, NLTK provides flexibility in tokenization.\"\n",
        "\n",
        "# NLTK Tokenization\n",
        "nltk_word_tokens = nltk.word_tokenize(text)  # FILL THIS\n",
        "nltk_sent_tokens = nltk.sent_tokenize(text)  # FILL THIS\n",
        "\n",
        "# spaCy Tokenization\n",
        "doc = nlp(text)\n",
        "spacy_tokens = [token.text for token in doc]\n",
        "\n",
        "print(\"NLTK Word Tokens:\", nltk_word_tokens)\n",
        "print(\"NLTK Sentence Tokens:\", nltk_sent_tokens)\n",
        "print(\"spaCy Tokens:\", spacy_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Regex Pattern Matching for Phone Number Detection**\n",
        "**Task:** Write a pattern using regex to find the phone nymber in the text."
      ],
      "metadata": {
        "id": "Sp7RCE4fIaVV"
      },
      "id": "Sp7RCE4fIaVV"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Example 2: Phone Number Extraction\n",
        "text_phones = \"Call me at +1-202-555-0173 or reach our office at (415) 123-4567.\"\n",
        "phone_pattern = r\"\"\"\n",
        "    (?:\\+1\\s?)?              # Optional +1 (country code for the US)\n",
        "    (?:\\(\\d{3}\\)|\\d{3})      # Area code (with or without parentheses)\n",
        "    [\\s\\-]?                  # Optional space or hyphen\n",
        "    \\d{3}                    # First 3 digits\n",
        "    [\\s\\-]?                  # Optional space or hyphen\n",
        "    \\d{4}                    # Last 4 digits\n",
        "\"\"\"\n",
        "phones = re.findall(phone_pattern, text_phones, re.VERBOSE)\n",
        "print(\"Detected Phone Numbers:\", phones)\n"
      ],
      "metadata": {
        "id": "XeSzDZ2-Icdv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ace853e4-d9ca-41e4-bf34-f21a26cb70a4"
      },
      "id": "XeSzDZ2-Icdv",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected Phone Numbers: ['202-555-0173', '(415) 123-4567']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. **Stopwords Filtering using NLTK**\n",
        "**Task:** Analyze movie reviews where stopwords are removed to focus on meaningful words."
      ],
      "metadata": {
        "id": "pdn94C-1cGgc"
      },
      "id": "pdn94C-1cGgc"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# nltk.download(\"stopwords\")\n",
        "# nltk.download(\"punkt\")\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# 🎬 Sample Movie Review\n",
        "review = \"\"\"The movie was absolutely amazing! The cinematography was stunning, and the characters were incredibly well-developed.\n",
        "However, the storyline felt a bit predictable at times, and some scenes were unnecessarily long. Overall, a great experience!\"\"\"\n",
        "\n",
        "# Tokenize words\n",
        "words = word_tokenize(review)\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stopwords.words('english')]\n",
        "\n",
        "# Output results\n",
        "print(\"Original Words:\", words)\n",
        "print(\"\\nFiltered (No Stopwords):\", filtered_words)\n"
      ],
      "metadata": {
        "id": "qFGnnFOjcM23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e5c013a-0693-44a3-c635-94845c1a0e41"
      },
      "id": "qFGnnFOjcM23",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['The', 'movie', 'was', 'absolutely', 'amazing', '!', 'The', 'cinematography', 'was', 'stunning', ',', 'and', 'the', 'characters', 'were', 'incredibly', 'well-developed', '.', 'However', ',', 'the', 'storyline', 'felt', 'a', 'bit', 'predictable', 'at', 'times', ',', 'and', 'some', 'scenes', 'were', 'unnecessarily', 'long', '.', 'Overall', ',', 'a', 'great', 'experience', '!']\n",
            "\n",
            "Filtered (No Stopwords): ['movie', 'absolutely', 'amazing', 'cinematography', 'stunning', 'characters', 'incredibly', 'however', 'storyline', 'felt', 'bit', 'predictable', 'times', 'scenes', 'unnecessarily', 'long', 'overall', 'great', 'experience']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. **Stemming Methods using NLTK**\n",
        "**Task:** Analyze legal and scientific terms to observe how different stemming algorithms behave."
      ],
      "metadata": {
        "id": "7UezErpHcu3x"
      },
      "id": "7UezErpHcu3x"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "\n",
        "# ⚖️ Sample Legal & Scientific Terms\n",
        "words = [\"arguing\", \"justification\", \"liable\", \"obligations\", \"classification\", \"microbiology\", \"evolutionary\", \"running\", \"happiness\"]\n",
        "\n",
        "# Initialize Stemmer Objects\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "\n",
        "# Apply Stemming\n",
        "porter_stems = [porter.stem(word) for word in words]\n",
        "lancaster_stems = [lancaster.stem(word) for word in words]\n",
        "\n",
        "# Output Results\n",
        "print(\"Original Words:\", words)\n",
        "print(\"\\nPorter Stemmer Results:\", porter_stems)\n",
        "print(\"\\nLancaster Stemmer Results:\", lancaster_stems)\n"
      ],
      "metadata": {
        "id": "Kpvzkd31d6Na",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cc4dc82-e464-4f8b-c6fc-65eb8a60ded8"
      },
      "id": "Kpvzkd31d6Na",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['arguing', 'justification', 'liable', 'obligations', 'classification', 'microbiology', 'evolutionary', 'running', 'happiness']\n",
            "\n",
            "Porter Stemmer Results: ['argu', 'justif', 'liabl', 'oblig', 'classif', 'microbiolog', 'evolutionari', 'run', 'happi']\n",
            "\n",
            "Lancaster Stemmer Results: ['argu', 'just', 'liabl', 'oblig', 'class', 'microbiolog', 'evolv', 'run', 'happy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. **Lemmatization Strategies using NLTK & spaCy**\n",
        "\n",
        "### NLTK’s WordNetLemmatizer\n",
        "**Task:** Lemmatize a political news headline to show how lemmatization helps retain the correct part of speech (POS) while normalizing words."
      ],
      "metadata": {
        "id": "ffIhEdRJeoI4"
      },
      "id": "ffIhEdRJeoI4"
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"wordnet\")\n",
        "# nltk.download(\"punkt\")\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# 📰 Sample News Headline\n",
        "headline = \"The senators debated the increasing regulations affecting technology companies.\"\n",
        "\n",
        "# Tokenize words\n",
        "words = word_tokenize(headline)\n",
        "\n",
        "# Initialize Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply Lemmatization (default without POS tagging)\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "print(\"Original Words:\", words)\n",
        "print(\"\\nLemmatized Words:\", lemmatized_words)\n"
      ],
      "metadata": {
        "id": "o4LlZqSxe086",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c741c72-6ab3-4216-fe96-d01074edeafd"
      },
      "id": "o4LlZqSxe086",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['The', 'senators', 'debated', 'the', 'increasing', 'regulations', 'affecting', 'technology', 'companies', '.']\n",
            "\n",
            "Lemmatized Words: ['The', 'senator', 'debated', 'the', 'increasing', 'regulation', 'affecting', 'technology', 'company', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### spaCy’s Built-in Lemmatizer"
      ],
      "metadata": {
        "id": "C61P_pdvfJlH"
      },
      "id": "C61P_pdvfJlH"
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process the same headline\n",
        "doc = nlp(headline)\n",
        "\n",
        "# Apply Lemmatization\n",
        "spacy_lemmatized = [token.lemma_ for token in doc]\n",
        "\n",
        "print(\"\\nspaCy Lemmatized Words:\", spacy_lemmatized)\n"
      ],
      "metadata": {
        "id": "4qPNWbqafWt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a351a1a7-8c5f-41a9-dbc2-9191387506de"
      },
      "id": "4qPNWbqafWt3",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "spaCy Lemmatized Words: ['the', 'senator', 'debate', 'the', 'increase', 'regulation', 'affect', 'technology', 'company', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. **Parsing & Chunking using NLTK**\n",
        "\n",
        "**Task:** Analyze legal contracts and job descriptions where parsing and chunking help extract meaningful phrases like noun phrases (NPs) or verb phrases (VPs)."
      ],
      "metadata": {
        "id": "nf-YB5yjfdl5"
      },
      "id": "nf-YB5yjfdl5"
    },
    {
      "cell_type": "code",
      "source": [
        "# 📜 Task: Extracting Key Phrases from Legal & Job Documents\n",
        "import nltk\n",
        "\n",
        "# nltk.download(\"punkt\")\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# 📜 Sample Legal Contract Text\n",
        "contract_text = \"The tenant shall pay the monthly rent before the 5th of each month.\"\n",
        "\n",
        "# Tokenize & POS Tagging\n",
        "words = nltk.word_tokenize(contract_text)\n",
        "pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "# Define a Chunking Grammar for Noun Phrases (NP)\n",
        "grammar = r\"NP: {<DT>?<JJ>*<NN>+}\"\n",
        "\n",
        "# Apply Chunking\n",
        "chunk_parser = nltk.RegexpParser(grammar)\n",
        "tree = chunk_parser.parse(pos_tags)\n",
        "\n",
        "# Display Results\n",
        "print(\"Chunked Tree:\")\n",
        "tree.pretty_print()\n"
      ],
      "metadata": {
        "id": "t9e4fOIuft4n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e560937-415b-4a42-bf2c-239df6ee950f"
      },
      "id": "t9e4fOIuft4n",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunked Tree:\n",
            "                                               S                                                                     \n",
            "    ___________________________________________|__________________________________________________________            \n",
            "   |       |        |       |      |      |    |          NP                      NP                      NP         \n",
            "   |       |        |       |      |      |    |     _____|______         ________|_________         _____|_____      \n",
            "shall/MD pay/VB before/IN the/DT 5th/CD of/IN ./. The/DT     tenant/NN the/DT monthly/JJ rent/NN each/DT     month/NN\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. **Exploring Hyponyms & Hypernyms using WordNet (NLTK)**\n",
        "\n",
        "**Task:** Hyponyms (specific terms) and hypernyms (general terms) in scientific and business domains, where hierarchical relationships between words are essential."
      ],
      "metadata": {
        "id": "GTmtGRjofqpB"
      },
      "id": "GTmtGRjofqpB"
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔍 Task: Explore Word Relationships in Science & Business\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# 🦁 Find Hypernyms & Hyponyms for \"lion\"\n",
        "word = \"lion\"\n",
        "synset = wordnet.synsets(word)[0]  # Selecting the first synset\n",
        "\n",
        "# Hypernyms (More General Category)\n",
        "hypernyms = synset.hypernyms()\n",
        "print(f\"Hypernyms (More General Concept) of '{word}':\")\n",
        "print([hypernym.name().split('.')[0] for hypernym in hypernyms])\n",
        "\n",
        "# Hyponyms (More Specific Types)\n",
        "hyponyms = synset.hyponyms()\n",
        "print(f\"\\nHyponyms (More Specific Types) of '{word}':\")\n",
        "print([hyponym.name().split('.')[0] for hyponym in hyponyms])\n"
      ],
      "metadata": {
        "id": "EELbOY0dgjsK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3773c2e3-6ac7-4cd8-b2a5-89fa1d441de4"
      },
      "id": "EELbOY0dgjsK",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hypernyms (More General Concept) of 'lion':\n",
            "['big_cat']\n",
            "\n",
            "Hyponyms (More Specific Types) of 'lion':\n",
            "['lionet', 'lioness', 'lion_cub']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdb5a06b",
      "metadata": {
        "id": "fdb5a06b"
      },
      "source": [
        "## **9. Named Entity Recognition (NER) with spaCy**\n",
        "**Task:** Extract named entities from a complex sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "dc7654be",
      "metadata": {
        "id": "dc7654be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1209a60c-7ddf-426d-c6f0-92494be75c63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities:\n",
            "1969 -> DATE\n",
            "Neil Armstrong -> PERSON\n",
            "first -> ORDINAL\n",
            "Moon -> PERSON\n",
            "Apollo 11 -> LAW\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"In 1969, Neil Armstrong became the first person to walk on the Moon during the Apollo 11 mission.\"\n",
        "\n",
        "doc = nlp(text)  # FILL THIS\n",
        "\n",
        "print(\"Named Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} -> {ent.label_}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}